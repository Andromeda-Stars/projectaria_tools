{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f77ab3",
   "metadata": {},
   "source": [
    "# MPS Tutorial\n",
    "This sample will show you how to use the Aria MPS data via the MPS apis.\n",
    "Please refer to the MPS wiki for more information about data formats and schemas\n",
    "\n",
    "### Notebook stuck?\n",
    "Note that because of Jupyter and Plotly issues, sometimes the code may stuck at visualization. We recommend **restart the kernels** and try again to see if the issue is resolved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifics for Google Colab\n",
    "google_colab_env = 'google.colab' in str(get_ipython())\n",
    "if google_colab_env:\n",
    "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
    "    !pip install projectaria-tools\n",
    "    !rm -rf ./projectaria_tools && git clone --filter=tree:0 --branch main https://github.com/facebookresearch/projectaria_tools.git\n",
    "    mps_sample_path = \"./projectaria_tools/data/mps_sample/\"\n",
    "else:\n",
    "    print(\"Using a pre-existing projectaria_tool github repository\")\n",
    "    mps_sample_path = \"../../data/mps_sample/\"\n",
    "\n",
    "import os\n",
    "vrsfile = os.path.join(mps_sample_path, \"sample.vrs\")\n",
    "# Trajectory and global points\n",
    "closed_loop_trajectory = os.path.join(mps_sample_path, \"trajectory\",\"closed_loop_trajectory.csv\")\n",
    "global_points = os.path.join(mps_sample_path, \"trajectory\", \"global_points.csv.gz\")\n",
    "\n",
    "#Eye Gaze\n",
    "eyegaze = os.path.join(mps_sample_path, \"eye_gaze\", \"eyegaze.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef125ae",
   "metadata": {},
   "source": [
    "## Load the trajectory, point cloud and eye gaze using the MPS apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projectaria_tools.core import data_provider, mps\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "import numpy as np\n",
    "# Create data provider and get T_device_rgb\n",
    "provider = data_provider.create_vrs_data_provider(vrsfile)\n",
    "# Since we want to display the position of the RGB camera, we are querying its relative location \n",
    "# from the device and will apply it to the device trajectory.\n",
    "T_device_RGB = provider.get_device_calibration().get_transform_device_sensor(\"camera-rgb\")\n",
    "\n",
    "## Load trajectory and global points\n",
    "mps_trajectory = mps.read_closed_loop_trajectory(closed_loop_trajectory)\n",
    "points = mps.read_global_point_cloud(global_points, mps.StreamCompressionMode.GZIP)\n",
    "\n",
    "## Load eyegaze\n",
    "eye_gazes = mps.read_eyegaze(eyegaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98903e99",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helper function to build the frustum\n",
    "def build_cam_frustum(transform_world_device):\n",
    "    points = np.array([[0, 0, 0], [0.5, 0.5, 1], [-0.5, 0.5, 1], [-0.5, -0.5, 1], [0.5, -0.5, 1]]) * 0.6\n",
    "    transform_world_rgb = transform_world_device.matrix() @ T_device_RGB.matrix()\n",
    "    points_transformed = (transform_world_rgb[:3, :3] @ points.T).T + transform_world_rgb[:3, 3]\n",
    "    return go.Mesh3d(x=points_transformed[:, 0], y=points_transformed[:, 1], z=points_transformed[:, 2],\n",
    "        i=[0, 0, 0, 0, 1, 1], j=[1, 2, 3, 4, 2, 3], k=[2, 3, 4, 1, 3, 4], showscale=False, visible=False,\n",
    "        colorscale=\"jet\", intensity=points[:, 2], opacity=1.0, hoverinfo='none')\n",
    "\n",
    "# Helper function to get nearest eye gaze for a timestamp\n",
    "def get_nearest_eye_gaze(eye_gazes, query_timestamp_ns):\n",
    "    return eye_gazes[min(range(len(eye_gazes)), key = lambda i: abs(eye_gazes[i].tracking_timestamp.total_seconds()*1e9 - query_timestamp_ns))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5d0aa",
   "metadata": {},
   "source": [
    "## Visualize the trajectory and point cloud in a 3D interactive plot\n",
    "* Load trajectory\n",
    "* Load global point cloud\n",
    "* Render dense trajectory (1Khz) as points.\n",
    "* Render subsampled 6DOF poses via camera frustum. Use calibration to transform RGB camera pose to world frame\n",
    "* Render subsampled point cloud\n",
    "\n",
    "_Please wait a minute for all the data to load. Zoom in to the point cloud and adjust your view. Then use the time slider to move the camera_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8502d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all world positions from the trajectory\n",
    "traj = np.empty([len(mps_trajectory), 3])\n",
    "for i in range(len(mps_trajectory)):\n",
    "    traj[i, :] = mps_trajectory[i].transform_world_device.translation()\n",
    "\n",
    "# Subsample trajectory for quick display\n",
    "skip = 1000\n",
    "mps_trajectory_subset = mps_trajectory[::skip]\n",
    "steps = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "# Load each pose as a camera frustum trace\n",
    "cam_frustums = [None]*len(mps_trajectory_subset)\n",
    "\n",
    "for i in range(len(mps_trajectory_subset)):\n",
    "    pose = mps_trajectory_subset[i]\n",
    "    cam_frustums[i] = build_cam_frustum(pose.transform_world_device)\n",
    "    timestamp = pose.tracking_timestamp.total_seconds()\n",
    "    step = dict(method=\"update\", args=[{\"visible\": [False] * len(cam_frustums) + [True] * 2}, {\"title\": \"Trajectory and Point Cloud\"},], label=timestamp,)\n",
    "    step[\"args\"][0][\"visible\"][i] = True  # Toggle i'th trace to \"visible\"\n",
    "    steps[i] = step\n",
    "cam_frustums[0].visible = True\n",
    "    \n",
    "# Filter the point cloud by inv depth and depth and load\n",
    "threshold_invdep = 0.001\n",
    "threshold_dep = 0.15\n",
    "skip = 1\n",
    "point_cloud = np.empty([len(points), 3])\n",
    "j = 0\n",
    "for i in range(0, len(points), skip):\n",
    "    if (points[i].inverse_distance_std < threshold_invdep and points[i].distance_std < threshold_dep):\n",
    "        point_cloud[j,:] = points[i].position_world\n",
    "        j=j+1\n",
    "point_cloud = point_cloud[:j,:]\n",
    "\n",
    "# Create slider to allow scrubbing and set the layout\n",
    "sliders = [dict(currentvalue={\"suffix\": \" s\", \"prefix\": \"Time :\"}, pad={\"t\": 5}, steps=steps,)]\n",
    "layout = go.Layout(sliders=sliders, scene=dict(bgcolor='lightgray', dragmode='orbit', aspectmode='data', xaxis_visible=False, yaxis_visible=False,zaxis_visible=False))\n",
    "\n",
    "# Plot trajectory and point cloud\n",
    "# We color the points by their z coordinate\n",
    "trajectory = go.Scatter3d(x=traj[:, 0], y=traj[:, 1], z=traj[:, 2], mode=\"markers\", marker={\"size\": 2, \"opacity\": 0.8, \"color\": \"red\"}, name=\"Trajectory\", hoverinfo='none')\n",
    "global_points = go.Scatter3d(x=point_cloud[:, 0], y=point_cloud[:, 1], z=point_cloud[:, 2], mode=\"markers\",\n",
    "    marker={\"size\" : 1.5, \"color\": point_cloud[:, 2], \"cmin\": -1.5, \"cmax\": 2, \"colorscale\": \"viridis\",},\n",
    "    name=\"Global Points\", hoverinfo='none')\n",
    "\n",
    "# draw\n",
    "plot_figure = go.Figure(data=cam_frustums + [trajectory, global_points], layout=layout)\n",
    "plot_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83cf57b",
   "metadata": {},
   "source": [
    "## Visualize eye gaze projection on an rgb image.\n",
    "* Load Eyegaze MPS output\n",
    "* Select a random RGB frame\n",
    "* Find the closest eye gaze data for the RGB frame\n",
    "* Project the eye gaze for the RGB frame by **using a fixed depth of 1m**.\n",
    "* Show the gaze cross on the RGB image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db594f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_stream_id = StreamId(\"214-1\")\n",
    "num_rgb_frames = provider.get_num_data(rgb_stream_id)\n",
    "rgb_frame = provider.get_image_data_by_index(rgb_stream_id, (int)(num_rgb_frames/2))\n",
    "assert rgb_frame[0] is not None, \"no rgb frame\"\n",
    "\n",
    "image = rgb_frame[0].to_numpy_array()\n",
    "capture_timestamp_ns = rgb_frame[1].capture_timestamp_ns\n",
    "eye_gaze = get_nearest_eye_gaze(eye_gazes, capture_timestamp_ns)\n",
    "\n",
    "# get projection function\n",
    "device_calibration = provider.get_device_calibration()\n",
    "cam_calibration = device_calibration.get_camera_calib(provider.get_label_from_stream_id(rgb_stream_id))\n",
    "assert cam_calibration is not None, \"no camera calibration\"\n",
    "\n",
    "depth_m = 1.0 # Select a fixed depth of 1m\n",
    "gaze_center_in_cpf = mps.get_eyegaze_point_at_depth(eye_gaze.yaw, eye_gaze.pitch, depth_m)\n",
    "transform_cpf_sensor = provider.get_device_calibration().get_transform_cpf_sensor(provider.get_label_from_stream_id(rgb_stream_id))\n",
    "gaze_center_in_camera = transform_cpf_sensor.inverse().matrix()@ np.hstack((gaze_center_in_cpf, 1)).T\n",
    "gaze_center_in_camera = gaze_center_in_camera[:3] / gaze_center_in_camera[3:]\n",
    "gaze_center_in_pixels = cam_calibration.project(gaze_center_in_camera)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Draw a cross at the projected gaze center location\n",
    "if gaze_center_in_pixels is not None:\n",
    "    ax.imshow(image);\n",
    "    ax.plot(gaze_center_in_pixels[0], gaze_center_in_pixels[1], '+', c=\"red\", mew=1, ms=20)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Eye gaze center projected to {gaze_center_in_pixels}, which is out of camera sensor plane.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
